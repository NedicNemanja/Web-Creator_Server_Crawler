***********************ΜΕΤΑΓΛΩΤΤΙΣΗ ΚΑΙ ΔΟΜΗ***********************************

Ο webcreator και το root_directory μαζι με το text_file.txt βρισκονται στο ιδιο
direcotry το οποιο περιλαμβανει αλλα 2 δirecotry.

1)Server οπου υπαρχει οτι σχτιζεται με τον Server καθως και το Makefile του.

2)Crawler αντιστοιχα για τον mycrawler Kαι το Makefile του.

**********************Εντολες για να τρεξουμε τις εφαρμογες*********************
RUN SERVER
./myhttpd  -p 8888 -c 9999 -t 4  -d ./root_directory/
(root_directory path relative)

SERVER ON BROWSER
localhost:8888/site0/page_name.html

SEND COMMANDS TO SERVER/Crawler
telnet localhost 9999/7777

RUN CRAWLER
./mycrawler -h localhost -p 8888 -c 7777 -t 4 -d ./save_dir/ http://linux01.com/site0/page0_230.html
(save_dir path relaltive to Crawler/ )
(starting_url: http://*/site0/page0_230.html)

******************** Σχετικα με Server *****************************************

Ξεκινοντας ανοιγει 2 listening socket. Ενα για να εξυπηρετει τα GET και ενα για
να υπακουει σε εντολες. Σε περιπτωση που η διευθυνση στην οποια προσπαθουμε να
κανουμε bind() ηδη χρησιμοποιειται, θα επαναχρησιμοποιηθει απο το socket μας 
με την βοηθεια της setsockopt().

Επειτα ξεκιναει τα threads. Το πρωτο thread κλειδωνει τον poll_lock mutex και
περιμενει(blocked) μεσω poll() στο serving_socket μεχρι να παρουσιαστει καποια
συνδεση. Ξεκλειδωνει mutex και μεσω αccept() (η οποια ειναι thread-safe)
αποδεχεται την συνδεση και την εξυπηρετει στο newsocket.

Για να εξυπηρετησει μια συνδεση διαβαζει τα δεδομενα σε chunks Mεγεθους READ_SIZE
απο το newsocket μεχρι να διαβαστει το blank line tou GET. Επειτα κανει parse to
GET kαι ανακαλυπτει το url της σελιδας που ζητηθηκε.
Σε περιπτωση που το GET request ηταν συντακτικα προβληματικο η δεν διαβαστηκε
ολοκληρο γιατι εκλεισε η συνδεση απροοπτα απλα θα αγνοηθει το συγκεκριμενο GET.
Συνεχιζοντας ο σερβερ θα απαντησει στο GET μεσω του newsocket με το καταλληλο
μηνυμα (200,403,404,500).
Τελειωνοντας ελευθερωνει το newsocket και ξαναμπαινει σε loop, το poll_lock ομως
ενδεχεται να ειναι κλειδωμενο απο αλλο thread το οποια εχει σειρα για συνδεση.

Να σημειωθει πως εαν αναμεσα στην αccept()  και την close(newsocket) το σημα
SIGPIPE (broken/closed socket) θα αγνοηθει καθως δεν θελουμε ολος ο σερβερ να
σταματησει με την λειτουργεια επειδη μια συνδεση απετυχε.

Καθε φορα που το thread προκειται να κανει poll() ενδεχομενως να μπλοκαρει
ελεγχουμε το SHUTDOWN_FLAG σε περιπτωση που δωθηκε εντολη τερματισμου του σερβερ
ετσι ωστε να ξερουν τα threads πως πρεπει αν τερματισουν.

Στο μεταξυ πισω στο main thread, θα εχει ανοιξει η συνδεση στο cοmmand_socket
η οποια περιμενει να συνδεθει καποιος πχ μεσω telnet και να αρχισει να δινει τις
εντολες. Οταν γινει η συνδεση σε ενα newsocket το προγραμμα θα διαβασει μια λεξη
για να καταλαβει για ποια εντολη προκεται. Στην περιπτωση που δεν καταφερει να
διαβασει επειδη εχει κλεισει η συνδεση επιστρεφεται μηνυμα πως η συνδεση εκλεισε
και περιμενουμε νεα συνδεση για εντολες αφου δεν υπαρχει αλλος τροπος να τερματι-
σουμε ομαλα εκτος απο το SHUTDOWN.
Εαν λαβουμε SHUTDOWN τοτε το SHUTDOWN_FLAG=1, τα τhreads θα παψουν να περιμενουν
επιπλεον συνδεσεις μεσω poll() ενω το Μαin thread θα καλεσει μια συναρτηση για να
κλεισει τα τhreads που ειναι blocked στελνοντας σημα SIGUSR1 και επειτα περιμενο-
ντας με pthread_join() να τερματισει το καθε thread του ThreadPool.

Επειτα ελευθερωνει ολους του file desc kαι τις δομες.


********************* Σχετικα με Crawler ***************************************

Ξεκινοντας δημιουργει μια ουρα στην οποια αποθηκευει τα links και βαζει εκει το
starting_url.
Επειτα δημιουργει το command_skcet για να ακουει εντολες, με τον ιδιο τροπο οπως
και ο σερβερ.

Ξεκιναει τα threads. Οταν ξεκιναει ενα thread ενημερωνει τον counter 
NUM_CRAWLING_THREADS. Αυτο χρησιμοποιειται για να ξερουμε αν εχουν τελειωσει ολα
τα thread με το crawling ετσι ωστε να μπορει να γινει η αναζητηση.

Επειτα το πρωτο τhread α προσπαθησει να αποκτησει ενα Link απο την ουρα κανοντας
lock to queue_lock mutex. Το πρωτο thread θα εχει σιγουρα το starting url για να
κανει crawl, αλλα τα επομενα ενδεχεται να μην βρουν link. Οποτε ειναι σημαντικο
να περιμενουν μεχρι να επιστρεψει καποιο response (eαν υπαρχει) και να ανακαλυ-
φθουν νεα links. Περιμενει λοιπον στο cond variable response_update μεχρι καποιο
αλλο thread να επιστρεψει αποτελεσματα.
Οταν καταφερει και παρει λινκ θα ενημερωσει τον PENDING_RESPONSES counter πως
εχει αναλαβει καποια σελιδα και οτι τα αλλα threads μπορουν να περιμενουν καποιο
αποτελσμα.
Θα προσπαθησει να κανει connect στον server εαν αποτυχει τοτε θα κλεισει αυτο,
και ολα τα υπολοιπα threads(chain reaction). Εαν πετυχει θα στειλει ενα GET
request και θα περιμενει απαντηση. ΑΝαλογα με την απαντηση (200,404,403,500)
ειτε θα αγνοησει τα αποτελεσμα και θα προσπαθησει να παρει επομενο λινκ, ειτε
θα το Kανει parse και θα το αποθηκευσει στον δισκο. Οταν εχει παρει το response
μειωνει τον PENDING_RESPONSES counter και κανει σignal to response_update, ετσι
ωστε το επομενο thread πoυ μπορει να περιμενει στο ιδιο cοndition να προσπαθησει
να παρει λινκ.
Εαν καποιο thread προσπαθησει να παρει λινκ και αποτυχει, ενω παραλληλα δεν
περιμενουμε αλλα responses τοτε θα τερματησει το ιδιο και τερματζοντας θα κανει
signal to response_update το οποιο με την σειρα του θα ξυπνησει το επομενο thread
που περιμενε να παρει λινκ και ετσι θα κελισουν ολα με ενα chain reaction.

Οι εντολες του crawler λειτουργουν με τον ιδιο τροπο οπως του σερβερ.
Οταν λαβει εντολη SHUTDOWN κανει το SHUTDOWN_FLAG=1 και καλει μια συναρητση
που κανει signal to response_update ετσι ωστε να ξυπνησουν ολα τα thread και αφου
εντοπισουν πως το SHUTDOWN_FLAG εγινε 1, να τερματισουν.



